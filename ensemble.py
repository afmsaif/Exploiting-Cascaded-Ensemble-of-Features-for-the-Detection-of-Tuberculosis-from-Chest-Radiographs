import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
import tqdm
from sklearn.model_selection import train_test_split
import pickle
from keras.regularizers import l2
from tensorflow.keras.models import Sequential
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn import svm
from sklearn import neural_network
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from google.colab import files
from sklearn.preprocessing import label_binarize
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.densenet import DenseNet169
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc
from keras.callbacks import EarlyStopping
import torch
# Extracted Deep Learning Features:
pickle_in_DL1 = open("/content/gdrive/My Drive/chest/pickle_files/Weights/densenet169/conv5_block32_concat_features1.pickle","rb")
pickle_in_DL2 = open("/content/gdrive/My Drive/chest/pickle_files/Weights/resnet50/conv5_block3_out_features1.pickle","rb")
pickle_in_DL3 = open("/content/gdrive/My Drive/chest/pickle_files/Weights/inceptionv3/mixed10_features1.pickle","rb")
XDL1 = pickle.load(pickle_in_DL1)
XDL2 = pickle.load(pickle_in_DL2)
XDL3 = pickle.load(pickle_in_DL3)

# Extracted HC Learning Features:
pickle_in_HC1 = open("/content/gdrive/My Drive/chest/pickle_files/BPPC_features.pickle","rb")
pickle_in_HC2 = open("/content/gdrive/My Drive/chest/pickle_files/IWBC_features.pickle","rb")
pickle_in_HC3 = open("/content/gdrive/My Drive/chest/pickle_files/LAP_features.pickle","rb")
pickle_in_HC4 = open("/content/gdrive/My Drive/chest/pickle_files/LMP_features.pickle","rb")
pickle_in_HC5 = open("/content/gdrive/My Drive/chest/pickle_files/MTP_features.pickle","rb")



XHC1 = pickle.load(pickle_in_HC1)
XHC2 = pickle.load(pickle_in_HC2)
XHC3 = pickle.load(pickle_in_HC3)
XHC4 = pickle.load(pickle_in_HC4)
XHC5 = pickle.load(pickle_in_HC5)

XHC1 = XHC1['featuresetsnew']
XHC2 = XHC2['featuresetsnew']
XHC3 = XHC3['featuresetsnew']
XHC4 = XHC4['featuresetsnew']
XHC5 = XHC5['featuresetsnew']

# Output:

YHC = np.transpose([[0]*331+[1]*331])
YDL = tf.keras.utils.to_categorical(YHC, 2)

print(np.ravel((YHC)))


all_indices = list(range(len(XHC1)))
train_index, test_index = train_test_split(all_indices,test_size = 0.2)


# 1st HC Feature set
pca1 = PCA(0.95)
pca1.fit(XHC1)
XHC1_train = pca1.transform(XHC1[train_index,:])
XHC1_test = pca1.transform(XHC1[test_index,:])
YHC1_train = YHC[train_index,:]
YHC1_test = YHC[test_index,:]

clf = LogisticRegression(random_state=0)

clf = clf.fit(XHC1_train,np.ravel(YHC[train_index]))
print(clf.classes_)
XHC1_full = np.concatenate((XHC1_train, XHC1_test))
score_HC1 = clf.predict_proba(XHC1_full)
ypred_HC1 = clf.predict(XHC1_test)
ypred_HC1 = label_binarize(ypred_HC1, classes=[0, 1])

accuracy_HC1 = accuracy_score(YHC[test_index], ypred_HC1)
print("Accuracy for BPPC Features: ",accuracy_HC1)
# 2nd Feature set
pca2 = PCA(0.95)
pca2.fit(XHC2)

XHC2_train = pca2.transform(XHC2[train_index,:])
XHC2_test = pca2.transform(XHC2[test_index,:])


clf = clf.fit(XHC2_train,np.ravel(YHC[train_index]))
print(clf.classes_)
XHC2_full = np.concatenate((XHC2_train, XHC2_test))
score_HC2 = clf.predict_proba(XHC2_full)
ypred_HC2 = clf.predict(XHC2_test)
accuracy_HC2 = accuracy_score(YHC[test_index], ypred_HC2)
print("Accuracy for IWBC Features: ",accuracy_HC2)


# 3rd Feature set
pca3 = PCA(0.95)
pca3.fit(XHC3)
XHC3_train = pca3.transform(XHC3[train_index,:])
XHC3_test = pca3.transform(XHC3[test_index,:])


clf = clf.fit(XHC3_train,np.ravel(YHC[train_index]))
print(clf.classes_)
XHC3_full = np.concatenate((XHC3_train, XHC3_test))
score_HC3 = clf.predict_proba(XHC3_full)
ypred_HC3 = clf.predict(XHC3_test)
accuracy_HC3 = accuracy_score(YHC[test_index], ypred_HC3)
print("Accuracy for LAP Features: ",accuracy_HC3)

# 4th Feature set
pca4 = PCA(0.95)
pca4.fit(XHC4)
XHC4_train = pca4.transform(XHC4[train_index,:])
XHC4_test = pca4.transform(XHC4[test_index,:])


clf = clf.fit(XHC4_train,np.ravel(YHC[train_index]))
print(clf.classes_)
XHC4_full = np.concatenate((XHC4_train, XHC4_test))
score_HC4 = clf.predict_proba(XHC4_full)
ypred_HC4 = clf.predict(XHC4_test)
accuracy_HC4 = accuracy_score(YHC[test_index], ypred_HC4)
print("Accuracy for LMP Features: ",accuracy_HC4)

# 5th Feature set
pca5 = PCA(0.95)
pca5.fit(XHC5)
XHC5_train = pca5.transform(XHC5[train_index,:])
XHC5_test = pca5.transform(XHC5[test_index,:])


clf = clf.fit(XHC5_train,np.ravel(YHC[train_index]))
print(clf.classes_)
XHC5_full = np.concatenate((XHC5_train, XHC5_test))
score_HC5 = clf.predict_proba(XHC5_full)
ypred_HC5 = clf.predict(XHC5_test)
accuracy_HC5 = accuracy_score(YHC[test_index], ypred_HC5)
print("Accuracy for MTP Features: ",accuracy_HC5)

# ensemble of HC features:
score_final_HC = np.zeros((662,2))
num0_HC = score_HC1[:,0]**(1/5)*score_HC2[:,0]**(1/5)*score_HC3[:,0]**(1/5)*score_HC4[:,0]**(1/5)*score_HC5[:,0]**(1/5)
den0_HC = (1-score_HC1[:,0])**(1/5)*(1-score_HC2[:,0])**(1/5)*(1-score_HC3[:,0])**(1/5)*(1-score_HC4[:,0])**(1/5)*(1-score_HC5[:,0])**(1/5)
score_final_HC[:,0] = num0_HC/(num0_HC + den0_HC)
num1_HC = score_HC1[:,1]**(1/5)*score_HC2[:,1]**(1/5)*score_HC3[:,1]**(1/5)*score_HC4[:,1]**(1/5)*score_HC5[:,1]**(1/5)
den1_HC = (1-score_HC1[:,1])**(1/5)*(1-score_HC2[:,1])**(1/5)*(1-score_HC3[:,1])**(1/5)*(1-score_HC4[:,1])**(1/5)*(1-score_HC5[:,1])**(1/5)
score_final_HC[:,1] = num1_HC/(num1_HC + den1_HC)

clf = LogisticRegression(random_state=0).fit(score_final_HC[:529,:],np.ravel(YHC[train_index]))
prob_scores_HC = clf.predict_proba(score_final_HC)

ypred_HC = clf.predict(score_final_HC[529:662,:])
ypred_HC_score = clf.predict_proba(score_final_HC[529:662,:])

accuracy_HC = accuracy_score(YHC[test_index], ypred_HC)


# ypred_HC = (score_final_HC[test_index,1] > 0.5).astype(int)
# accuracy_HC = accuracy_score(YHC1_test, ypred_HC)

#cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
#score = cross_val_score(clf, score_final_HC , YHC, cv=cv)
#print(score)
print("Accuracy for HC Features: ",accuracy_HC)


# DL Feature Extraction

# 1st Feature:
XDL1_full = np.concatenate((XDL1[train_index,:],XDL1[test_index]))
model_transfer1 = Sequential()
model_transfer1.add(GlobalAveragePooling2D(input_shape=XDL1.shape[1:]))
model_transfer1.add(Dropout(0.3))
model_transfer1.add(Dense(100, activation='relu'))
model_transfer1.add(Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
model_transfer1.add(Dense(2, activation='softmax'))
model_transfer1.summary()

model_transfer1.save('/content/gdrive/My Drive/chest/my_model.h5') 


checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_acc',
    mode='max',
    save_best_only=True)

model_transfer1.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              metrics=['accuracy', 'AUC'])
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
history = model_transfer1.fit(XDL1[train_index,:], YDL[train_index], batch_size=128, epochs=50, validation_data = (XDL1[test_index,:], YDL[test_index]),  
                             callbacks=[model_checkpoint_callback, es], verbose=1, shuffle=True)
def plot_acc_loss(history):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy for Densenet-169')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
 
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss for Densenet-169')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')
    plt.show()
 
plot_acc_loss(history)

score_DL1 = model_transfer1.predict(XDL1_full)


accuracy_DL1 = model_transfer1.evaluate(XDL1[test_index,:], YDL[test_index], verbose=1)
print("Accuracy for conv5_block32_concat_features1 Features: ", accuracy_DL1[1])



# 2nd Feature:
XDL2_full = np.concatenate((XDL2[train_index,:],XDL2[test_index]))
model_transfer2 = Sequential()
model_transfer2.add(GlobalAveragePooling2D(input_shape=XDL2.shape[1:]))
model_transfer2.add(Dropout(0.3))
model_transfer2.add(Dense(100, activation='relu'))
model_transfer2.add(Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
model_transfer2.add(Dense(2, activation='softmax'))
model_transfer2.summary()


model_transfer2.save('/content/gdrive/My Drive/chest/my_model_2.h5') 

checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_acc',
    mode='max',
    save_best_only=True)
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
model_transfer2.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              metrics=['accuracy', 'AUC'])

history = model_transfer2.fit(XDL2[train_index,:], YDL[train_index], batch_size=128, epochs=50, validation_data = (XDL2[test_index,:], YDL[test_index]),  
                             callbacks=[model_checkpoint_callback, es], verbose=1, shuffle=True)
def plot_acc_loss(history):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy for inceptionv3 ')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
 
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss for inceptionv3 ')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')
    plt.show()
 
plot_acc_loss(history)

score_DL2 = model_transfer2.predict(XDL2_full)
accuracy_DL2 = model_transfer2.evaluate(XDL2[test_index,:], YDL[test_index], verbose=1)
print("Accuracy for conv5_block32_concat_features1 Features: ", accuracy_DL2[1])


# 3rd Feature:
XDL3_full = np.concatenate((XDL3[train_index,:],XDL3[test_index]))
model_transfer3 = Sequential()
model_transfer3.add(GlobalAveragePooling2D(input_shape=XDL3.shape[1:]))
model_transfer3.add(Dropout(0.2))
model_transfer3.add(Dense(100, activation='relu'))
model_transfer3.add(Dense(2, activation='softmax'))
model_transfer3.summary()


checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_acc',
    mode='max',
    save_best_only=True)
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)
model_transfer3.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-3),
              metrics=['accuracy', 'AUC'])

history = model_transfer3.fit(XDL3[train_index,:], YDL[train_index], batch_size=128, epochs=50, validation_data = (XDL3[test_index,:], YDL[test_index]),  
                             callbacks=[model_checkpoint_callback, es], verbose=1, shuffle=True)
def plot_acc_loss(history):
    fig = plt.figure(figsize=(10,5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('model accuracy for Inception-v3')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
 
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss for Inception-v3 ')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper right')
    plt.show()
 
plot_acc_loss(history)

score_DL3 = model_transfer3.predict(XDL3_full)
accuracy_DL3 = model_transfer3.evaluate(XDL3[test_index,:], YDL[test_index], verbose=1)
print("Accuracy for activation_93_features1 Features: ", accuracy_DL3[1])



# ensemble of DL methods
score_final_DL = np.zeros((662,2))
num0_DL = score_DL1[:,0]**(1/2)*score_DL2[:,0]**(1/2)
den0_DL = (1-score_DL1[:,0])**(1/2)*(1-score_DL2[:,0])**(1/2)
score_final_DL[:,0] = num0_DL/(num0_DL + den0_DL)
num1_DL = score_DL1[:,1]**(1/2)*score_DL2[:,1]**(1/2)
den1_DL = (1-score_DL1[:,1])**(1/2)*(1-score_DL2[:,1])**(1/2)
score_final_DL[:,1] = num1_DL/(num1_DL + den1_DL)


clf = LogisticRegression(random_state=0).fit(score_final_DL[:529,:],np.ravel(YHC[train_index]))
prob_scores_DL = clf.predict_proba(score_final_DL)
ypred_DL = clf.predict(score_final_DL[529:662,:])
ypred_DL_score = clf.predict_proba(score_final_DL[529:662,:])


accuracy_DL = accuracy_score(YHC[test_index], ypred_DL)


print("Accuracy for DL Features: ", accuracy_DL)


# ensemble of DL methods
score_final_DL_HC = np.zeros((662,2))
num0_DL_HC = score_final_HC[:,0]**(1/2)*score_final_DL[:,0]**(1/2)
den0_DL_HC = (1-score_final_HC[:,0])**(1/2)*(1-score_final_DL[:,0])**(1/2)
score_final_DL_HC[:,0] = num0_DL_HC/(num0_DL_HC + den0_DL_HC)
num1_DL_HC = score_final_HC[:,1]**(1/2)*score_final_DL[:,1]**(1/2)
den1_DL_HC = (1-score_final_HC[:,1])**(1/2)*(1-score_final_DL[:,1])**(1/2)
score_final_DL_HC[:,1] = num1_DL_HC/(num1_DL_HC + den1_DL_HC)


clf = LogisticRegression(random_state=0).fit(score_final_DL_HC[:529,:],np.ravel(YHC[train_index]))
prob_scores_HC_DL = clf.predict_proba(score_final_DL_HC)
ypred_DL_HC = clf.predict(score_final_DL_HC[529:662,:])
ypred_DL_HC_score = clf.predict_proba(score_final_DL_HC[529:662,:])
fpr, tpr, thresholds = roc_curve(YHC[test_index], ypred_DL_HC_score[:,1])
roc_auc_DL_HC = auc(fpr, tpr)
fpr_DL, tpr_DL, thresholds_DL = roc_curve(YHC[test_index], ypred_DL_score[:,1])
roc_auc_DL = auc(fpr_DL, tpr_DL)
fpr_HC, tpr_HC, thresholds_HC = roc_curve(YHC[test_index], ypred_HC_score[:,1])
roc_auc_HC = auc(fpr_HC, tpr_HC)
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='darkorange',
         lw=lw, label='HE and DL Ensemble(area = %0.2f)' % roc_auc_DL_HC)
plt.plot(fpr_DL, tpr_DL, color='blue',
         lw=lw, label='DL Ensemble (area = %0.2f)' % roc_auc_DL)
plt.plot(fpr_HC, tpr_HC, color='red',
         lw=lw, label='HE Ensemble (area = %0.2f)' % roc_auc_HC)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for different Ensemble Techniques')
plt.legend(loc="lower right")
plt.show()

accuracy_DL_HC = accuracy_score(YHC[test_index], ypred_DL_HC)


print("Accuracy for DL & HC Features: ", accuracy_DL_HC)
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2,5,i+1), plt.title(i)
    plt.imshow(np.mean(X_train[y_train==i],axis=0),cmap='gray')
plt.suptitle('Mean images for each digit')